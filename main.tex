\documentclass{article}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{lipsum}
%for long table
\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
 {}

\newenvironment{figurehere}
 {\def\@captype{figure}}
 {}
\makeatother
\usepackage{longtable}

\usepackage{graphicx}
\usepackage{abstract}
\renewcommand{\abstractname}{}
\renewcommand{\absnamepos}{empty}
\usepackage{zhlipsum}
\usepackage{color}
\usepackage{multirow}
\usepackage{cuted}
\graphicspath{{picture/}}
\usepackage[left=2.5cm,right=1.97cm,top=2.5cm,bottom=2.5cm]{geometry}%设置页边距
\renewcommand{\baselinestretch}{1.25}%行间距
%\usepackage[hidelinks,urlcolor=black,linkcolor=black]{hyperref}%引入超链接包，否则会出现Undefined sequence
\usepackage[hidelinks]{hyperref}
%[colorlinks,urlcolor=black,linkcolor=black]去除超链接中的颜色框
\usepackage{amssymb}%数学符号
\usepackage{amsmath}%数学公式
\usepackage{booktabs}%设置三线表的线粗细
\usepackage{array}%table
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
%\usepackage{cite}
% \ctexset{section={format={\zihao{3} \heiti \bfseries}},
% bibname={\zihao{-4} \heiti \bfseries 参考文献}}

\newcommand{\upcite}[1]{\textsuperscript{\textsuperscript{\cite{#1}}}}
\usepackage{caption}
\usepackage{geometry}
\title{mrc}
%\footnotetext{ 投稿日期: 2020-07-19 \\
%\hspace*{1.8em}作者简介: 孙相会（1997-），男，硕士，研究方向为自然语言处理，E-mail: 2357094733@qq.com}
%黑体2号 在标题那页插入脚注


%\author{\zihao{-4} \songti 孙相会 \\ 东北大学 计算机科学与工程学院，沈阳 110169}
\date{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%












\begin{document}
    \maketitle %生成title,author,date
	\begin{abstract}
		Leave the summary in the end
	\end{abstract}
%----------------------正文-----------------------

\input{introduction.tex}

\input{outline.tex}

\section{Neural machine reading comprehension model}
\subsection{general architecture}
Machine reading comprehension generally needs the following processes:

\begin{enumerate}
	\item Representing the unstructured data in the text form of paragraphs and questions as a form that can be processed by the computer.
	\item In order to enhance the semantic representation of a paragraph or question, it is necessary to enable a word in question or paragraph to pay attention to its contextual information.
	\item According to the question, retrieve the part of the paragraph that is most relevant to the question.
	\item Summarize the answers from the retrieved article fragments.
\end{enumerate}

It can be seen from the whole process that each step has a clear purpose and corresponds to a certain layer in the neural network. The overall framework of 
deep learning model for MRC tasks mainly includes the following layers: word embedding layer, feature extraction layer, interaction layer and answer output layer. 
The word embedding layer corresponds to step 1, which embeds paragraph and question into a low dimensional vector space, and uses each vector to represent each word; 
The feature extraction layer corresponds to step 2, and its function is to encode the semantic information of paragraph and question, so that each word can pay attention to its context;
The interaction layer corresponds to step 3, whose function is to fuse the semantic information of the paragraph with the semantic information of the question, so that 
the model can learn the most relevant part of paragraph with the question; 
The answer output layer corresponds to step 4, and the goal is to find the answer to the question from the paragraph.

\subsubsection{Word embedding layer}
How to express text into a form that can be processed by computers while effectively utilizing the semantic relationship between words 
has always been one of key issues in NLP field. The early one hot form coding uses a binary vector to represent words, but there is the problem of 
data sparsity and dimensional disaster with the increase of the number of words. In addition, this form of coding could not represent the semantic 
relationship between words

\begin{equation}
	\pi=x_i
\end{equation}


\end{document}

