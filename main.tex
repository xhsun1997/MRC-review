\documentclass{article}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage{lipsum}
%for long table
\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
 {}

\newenvironment{figurehere}
 {\def\@captype{figure}}
 {}
\makeatother
\usepackage{longtable}

\usepackage{graphicx}
\usepackage{abstract}
\renewcommand{\abstractname}{}
\renewcommand{\absnamepos}{empty}
\usepackage{zhlipsum}
\usepackage{color}
\usepackage{multirow}
\usepackage{cuted}
\graphicspath{{picture/}}
\usepackage[left=2.5cm,right=1.97cm,top=2.5cm,bottom=2.5cm]{geometry}%设置页边距
\renewcommand{\baselinestretch}{1.25}%行间距
%\usepackage[hidelinks,urlcolor=black,linkcolor=black]{hyperref}%引入超链接包，否则会出现Undefined sequence
\usepackage[hidelinks]{hyperref}
%[colorlinks,urlcolor=black,linkcolor=black]去除超链接中的颜色框
\usepackage{amssymb}%数学符号
\usepackage{amsmath}%数学公式
\usepackage{booktabs}%设置三线表的线粗细
\usepackage{array}%table
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
%\usepackage{cite}
% \ctexset{section={format={\zihao{3} \heiti \bfseries}},
% bibname={\zihao{-4} \heiti \bfseries 参考文献}}

\newcommand{\upcite}[1]{\textsuperscript{\textsuperscript{\cite{#1}}}}
\usepackage{caption}
\usepackage{geometry}
\title{mrc}
%\footnotetext{ 投稿日期: 2020-07-19 \\
%\hspace*{1.8em}作者简介: 孙相会（1997-），男，硕士，研究方向为自然语言处理，E-mail: 2357094733@qq.com}
%黑体2号 在标题那页插入脚注


%\author{\zihao{-4} \songti 孙相会 \\ 东北大学 计算机科学与工程学院，沈阳 110169}
\date{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%












\begin{document}
    \maketitle %生成title,author,date
	\begin{abstract}
		Leave the summary in the end
	\end{abstract}
%----------------------正文-----------------------

\input{introduction.tex}

\input{outline.tex}

\section{Neural machine reading comprehension model}

\input{generalArchitecture.tex}

\input{classicMRCModel.tex}

\subsection{MRC based on pretrain}
Pre-training model has received great attention in NLP field in recent years. The pre-training method originates from the concept of 
transfer learning. Firstly, the model is pre-trained on other related tasks so that the model can learn some knowledge, then further optimization is made 
on the target tasks to realize the transfer of knowledge learned by the model. For NLP domain, the pre-training process is to learn the general knowledge 
representation on a large number of text data. The comparison between the structure based on pre-training and the traditional structure is shown in figure 2. 

\subsubsection{Pre-training model}
OpenAI proposes a generative pre-training(GPT) model, which uses the decoder of transformer as feature extractor. The objective function of pre-training stage is 
the objective function of unidirectional language model, see formula 1. 

GPT belongs to autoregressive language model. The disadvantage of autoregressive language model is that it cannot predict a word by using the context information 
of the word at the same time due to the nature of autoregressive. Devlin et al. proposed a very powerful pre-training model BERT. The difference with GPT lies in 
the pre-training method adopts denoising autoencoder, which randomly masks some words and obtains the probability distribution of masking positions at the output layer, 
and lets the model predict the masked word according to the context of the masked position. This mechanism also called masked language model(MLM) or bidirectional language model. 
In addition to MLM task, BERT also uses Next Sentence Prediction(NSP) task to make the model perform better in downstream tasks such text entailment, question answering, which need to judge 
the semantic relationship between the two sentences. It can be seen that the pre-training process of BERT is essentially a multi-task learning process, and the semantic expression 
ability of BERT is improved through MLM and NSP tasks. The performance of BERT on the most classic machine reading comprehension dataset SQuAD has surpassed that of human beings. 
BERT opened the era of pre-training models in NLP domain. Since then, many more powerful pre-training models have been proposed one after another. 



\begin{equation}
	\pi=x_is
\end{equation}


\end{document}

